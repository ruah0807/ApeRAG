volumes:
  aperag-postgres-data: {}
  aperag-qdrant-data: {}
  aperag-redis-data: {}
  aperag-es-data: {}
  aperag-neo4j-data: {}
  aperag-shared-data: {}

services:
  # ==============================================
  # Application Services (Default startup, excluded from infra)
  # ==============================================
  api: &api
    build:
      context: .
      dockerfile: ./Dockerfile
    image: ${REGISTRY:-docker.io}/apecloud/aperag:${VERSION:-v0.0.0-nightly}
    container_name: aperag-api
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      es:
        condition: service_healthy
    volumes:
      - ~/.cache:/root/.cache
      - aperag-shared-data:/shared
    env_file:
      - .env
      - envs/docker.env.overrides
    environment:
      - DOCRAY_HOST=${DOCRAY_HOST}
    ports:
      - "8000:8000"
    command: ["/app/scripts/entrypoint.sh", "/app/scripts/start-api.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || curl -f http://localhost:8000/docs || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12  # 12 * 10s = 120s (2 minutes)
      start_period: 30s  # Give API time to start up before first check

  frontend:
    build:
      context: ./web
      dockerfile: ./Dockerfile
    image: ${REGISTRY:-docker.io}/apecloud/aperag-frontend:${VERSION:-v0.0.0-nightly}
    container_name: aperag-frontend
    depends_on:
      api:
        condition: service_healthy
    env_file:
      - web/deploy/env.local.template
    environment:
      - API_SERVER_ENDPOINT=http://api:8000
      - API_SERVER_BASE_PATH=/api/v1
    ports:
      - "3001:3000"

  celeryworker:
    image: ${REGISTRY:-docker.io}/apecloud/aperag:${VERSION:-v0.0.0-nightly}
    build:
      context: .
      dockerfile: ./Dockerfile
    container_name: aperag-celeryworker
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      es:
        condition: service_healthy
    volumes:
      - ~/.cache:/root/.cache
      - ./resources:/data/resources
      - aperag-shared-data:/shared
    env_file:
      - .env
      - envs/docker.env.overrides
    environment:
      - NODE_IP=aperag-celeryworker
      - DOCRAY_HOST=${DOCRAY_HOST}
      - APERAG_API_BASE_URL=http://aperag-api:8000
    command: ["/app/scripts/entrypoint.sh", "/app/scripts/start-celery-worker.sh"]

  celerybeat:
    image: ${REGISTRY:-docker.io}/apecloud/aperag:${VERSION:-v0.0.0-nightly}
    build:
      context: .
      dockerfile: ./Dockerfile
    container_name: aperag-celerybeat
    env_file:
      - .env
      - envs/docker.env.overrides
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - NODE_IP=aperag-celerybeat
    command: ["/app/scripts/entrypoint.sh", "/app/scripts/start-celery-beat.sh"]

  flower:
    image: ${REGISTRY:-docker.io}/apecloud/aperag:${VERSION:-v0.0.0-nightly}
    build:
      context: .
      dockerfile: ./Dockerfile
    container_name: aperag-flower
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - .env
      - envs/docker.env.overrides
    ports:
      - "5555:5555"
    environment:
      - NODE_IP=aperag-flower
    command: ["/app/scripts/entrypoint.sh", "/app/scripts/start-celery-flower.sh"]

  # ==============================================
  # Infrastructure Services (always available)
  # ==============================================
  postgres:
    image: ${REGISTRY:-docker.io}/apecloud/pgvector:pg16
    container_name: aperag-postgres
    volumes:
      - aperag-postgres-data:/var/lib/postgresql/data
    ports:
      - "127.0.0.1:6000:5432"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=aperag
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d aperag"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: ${REGISTRY:-docker.io}/apecloud/redis:6
    container_name: aperag-redis
    volumes:
      - aperag-redis-data:/data
    ports:
      - "127.0.0.1:6100:6379"
    command: redis-server --requirepass password
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a password ping | grep PONG"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s

  qdrant:
    image: ${REGISTRY:-docker.io}/apecloud/qdrant:v1.13.4
    container_name: aperag-qdrant
    volumes:
      - aperag-qdrant-data:/qdrant/storage
    ports:
      - "127.0.0.1:6200:6333"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 6333 || timeout 3 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  es:
    image: ${REGISTRY:-docker.io}/apecloud/elasticsearch:8.8.2
    container_name: aperag-es
    ports:
      - "127.0.0.1:6300:9200"
    environment:
      - "discovery.type=single-node"
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - "xpack.security.enabled=false"
    dns:
      - 8.8.8.8
      - 8.8.4.4
    volumes:
      - aperag-es-data:/usr/share/elasticsearch/data
      - ./scripts/init-es.sh:/usr/share/elasticsearch/bin/init-es.sh
    healthcheck:
      test: ["CMD-SHELL", "bash /usr/share/elasticsearch/bin/init-es.sh check"]
      interval: 10s
      timeout: 5s
      retries: 12 # 12 * 10s = 120s (2 minutes)
      start_period: 10s # Give ES some time to start up before first check
    command: bash /usr/share/elasticsearch/bin/init-es.sh
    restart: on-failure

  jaeger:
    image: jaegertracing/all-in-one:1.60
    container_name: aperag-jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # HTTP collector
      - "14250:14250"  # gRPC collector
      - "6831:6831/udp"  # UDP agent
      - "6832:6832/udp"  # UDP agent
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=info
    restart: unless-stopped
    profiles: ["jaeger"]

  # ==============================================
  # Example Models
  # ==============================================
  embedding-service:
    image: vllm/vllm-openai:latest
    container_name: aperag-embedding-service
    ports:
      - "7000:8000" # vLLM 기본 포트 8000을 호스트 7000으로 매핑
    volumes:
      - /home/pps-nipa/NIQ/ruah/backend/storage/models/embedding/embedding_qwen3_4b:/model
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]
    # 임베딩 전용 설정 (GPU 메모리 등은 상황에 맞게 조절하세요)
    command: >
      --model /model
      --served-model-name qwen3-embedding
      --max-model-len 8192
      --gpu-memory-utilization 0.6
      --trust-remote-code
              
  llm-service:
    image: vllm/vllm-openai:latest # OpenAI API와 호환되는 방식으로 로컬 모델을 서빙하기 위해
    container_name: aperag-llm-service
    ports:
      - "7100:7100"
    volumes:
      # 호스트의 LLM 모델 경로 마운트
      - /home/pps-nipa/NIQ/ruah/backend/storage/models/llm/Gemma3-27B:/model
    shm_size: 16gb  # vLLM은 충분한 공유 메모리가 필요합니다
    # vLLM 실행 명령 (모델명, 컨텍스트 길이 등 설정)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1','2']  
              capabilities: [gpu]
    command: >
      --model /model
      --served-model-name gemma3-27b
      --max-model-len 32768
      --gpu-memory-utilization 0.9
      --trust-remote-code
      --port 7100
      --tensor-parallel-size 2  
    #  중요: GPU 2개를 병렬로 사용 (device_ids: ['1','2']등 여러개로되어있는경우 )


  # ==============================================
  # Optional Services
  # ==============================================
  neo4j:
    image: neo4j:5.26.5-enterprise
    container_name: aperag-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
      - NEO4J_apoc_export_file_enabled=true
    volumes:
      - aperag-neo4j-data:/data
    profiles: ["neo4j"]

  # ==============================================
  # DocRay Services (existing profiles)
  # ==============================================
  docray:
    image: ${REGISTRY:-docker.io}/apecloud/doc-ray:${DOCRAY_VERSION:-v0.2.0}
    container_name: aperag-docray
    ports:
      - "8265:8265"
      - "8639:8639"
    profiles: ["docray"]
    environment:
      - STANDALONE_MODE=true
    deploy:
      resources:
        reservations:
          memory: "8G"

  docray-gpu:
    image: ${REGISTRY:-docker.io}/apecloud/doc-ray:${DOCRAY_VERSION:-v0.2.0}
    container_name: aperag-docray-gpu
    ports:
      - "8265:8265"
      - "8639:8639"
    profiles: ["docray-gpu"]
    environment:
      - STANDALONE_MODE=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
